{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализация алгоритма оптимизатора Adam (Adaptive Moment Estimation) без использования сторонних библиотек, таких как PyTorch или TensorFlow, представляет собой интересное задание. Adam является одним из наиболее популярных оптимизаторов в глубоком обучении, который сочетает в себе преимущества двух других популярных методов: стохастического градиентного спуска (SGD) и моменного метода (Momentum).\n",
    "\n",
    "Adam использует два скользящих средних для каждого параметра: одно для градиентов (среднее квадрат градиентов) и другое для скоростей обновления параметров (среднее скорости). Эти средние используются для адаптивного изменения скорости обучения на каждом шаге оптимизации.\n",
    "\n",
    "Алгоритм Adam:\n",
    "    Инициализация: Инициализируем параметры модели и скользящие средние для градиентов и скоростей обновления.\n",
    "    Обновление скользящих средних: На каждом шаге оптимизации обновляем скользящие средние градиентов и скоростей обновления.\n",
    "    Вычисление скорости обновления: Используем скользящие средние для вычисления скорости обновления.\n",
    "    Обновление параметров: Применяем скорость обновления к текущим параметрам модели.\n",
    "    \n",
    "Код реализации:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't multiply sequence by non-int of type 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 42\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Выполнение нескольких шагов оптимизации\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m---> 42\u001b[0m     s1, s2, t \u001b[38;5;241m=\u001b[39m \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpdated parameters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparams\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 16\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, s1, s2, t, learning_rate, beta1, beta2, epsilon)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03mparams: список параметров модели\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124;03mgrads: список градиентов для каждого параметра\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;124;03mepsilon: небольшое число для стабильности\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Обновление скользящих средних\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m s1_t \u001b[38;5;241m=\u001b[39m beta1 \u001b[38;5;241m*\u001b[39m s1 \u001b[38;5;241m+\u001b[39m \u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mgrads\u001b[49m\n\u001b[0;32m     17\u001b[0m s2_t \u001b[38;5;241m=\u001b[39m beta2 \u001b[38;5;241m*\u001b[39m s2 \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2) \u001b[38;5;241m*\u001b[39m (grads \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Коррекция скользящих средних\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: can't multiply sequence by non-int of type 'float'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def adam(params, grads, s1, s2, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "    \"\"\"\n",
    "    params: список параметров модели\n",
    "    grads: список градиентов для каждого параметра\n",
    "    s1: первый скользящий средний для градиентов\n",
    "    s2: второй скользящий средний для скоростей обновления\n",
    "    t: текущий шаг обучения\n",
    "    learning_rate: скорость обучения\n",
    "    beta1: коэффициент для первого скользящего среднего\n",
    "    beta2: коэффициент для второго скользящего среднего\n",
    "    epsilon: небольшое число для стабильности\n",
    "    \"\"\"\n",
    "    # Обновление скользящих средних\n",
    "    s1_t = beta1 * s1 + (1 - beta1) * grads\n",
    "    s2_t = beta2 * s2 + (1 - beta2) * (grads ** 2)\n",
    "    \n",
    "    # Коррекция скользящих средних\n",
    "    s1_corr = s1_t / (1 - beta1 ** (t + 1))\n",
    "    s2_corr = s2_t / (1 - beta2 ** (t + 1))\n",
    "    \n",
    "    # Вычисление скорости обновления\n",
    "    m_t = learning_rate * s1_corr / (np.sqrt(s2_corr) + epsilon)\n",
    "    \n",
    "    # Обновление параметров\n",
    "    for i, param in enumerate(params):\n",
    "        param -= m_t[i]\n",
    "    \n",
    "    # Обновление скользящих средних и шага обучения\n",
    "    return s1, s2, t + 1\n",
    "\n",
    "# Пример использования\n",
    "params = [np.array([1.0]), np.array([-1.0])]\n",
    "grads = [np.array([0.1]), np.array([-0.2])]\n",
    "s1 = np.zeros_like(grads)\n",
    "s2 = np.zeros_like(grads)\n",
    "t = 0\n",
    "\n",
    "# Выполнение нескольких шагов оптимизации\n",
    "for _ in range(10):\n",
    "    s1, s2, t = adam(params, grads, s1, s2, t)\n",
    "    print(f\"Updated parameters: {params}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Этот код демонстрирует базовую реализацию алгоритма Adam. Он инициализирует параметры модели, градиенты, скользящие средние и шаг обучения, а затем применяет алгоритм Adam для обновления параметров модели на протяжении нескольких шагов."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
